第一章
1.机器学习应用实例：
  -数据挖掘
  -完成我们无法编程完成的工作（计算机视觉、自然语言处理）
  -私人定制程序
  -对人类学习方式有更深入的理解

2.什么是机器学习？
  -Arthur Samuel定义：让计算机能够在未被明确编程的情况下自我进行学习的研究；
  -Tom Michell定义：一个在经验E中学习，解决任务T，被使用P指标来量化表现的计算机程序；
  对于Samuel的跳棋程序来说，经验E指跳棋自对弈的成千上万把比赛，任务T指进行跳棋对弈，指标P指与新对手对弈的胜率（新对手！）；
  
3.机器学习算法
  -主要是监督学习与无监督学习；
  -其他的还有强化学习和推荐系统；
  
4.监督学习
  -监督学习是包含了正确答案的数据集，使用这个算法的目的是给出更多的正确答案；
  -输出是连续的：回归问题；输出是离散的：分类问题；
  
5.无监督学习
  -无监督学习中数据集没有标签，使用该算法将数据分类成不同的簇；
  -聚类算法：将属性相近的个体归类到一起；
  -应用：鸡尾酒派对问题算法；

第二章 单变量线性回归
1.模型描述
  -m通常用来代表训练集的数量，x代表输入变量，y代表输出变量，h表示假设函数；

2.代价函数
  -解决最小化问题，选择参数使得残差平方极小；
  -代价函数写法；
  
3.代价函数（一）
  -线性回归函数的优化目标；
  
4.代价函数（二）
  -带一个参数的代价函数图像为二维的；
  -带两个参数的代价函数图像为三维的；
  -代价函数的最小值时的参数取值是回归时的最优取值；
  
5.梯度下降
  -在每一个步长点找到斜率最小的方向进行下降；
  -进行梯度下降需要起始点；
  -代价函数前的系数称为学习率；
  -梯度下降的最重要一点是同步更新；
  -学习率小，更新速率小，但一定程度上能保证参数收敛；学习率大，更新速率大，但很可能参数不收敛；
  -在收敛至局部最优点附近时，更新速率会越来越小，这是由于导数值越来越接近0导致的；

6.线性回归中的梯度下降
  -梯度下降代价函数是一个凸函数，有且只有一个全局最优解，而且使用梯度下降的方法总能收敛到此处；
  -像这样遍历了整个数据集的梯度下降法，我们称之为“Batch梯度下降法”；
  
第三章 多变量线性回归
1.多功能
  -此时输入的变量是一个向量；
  -在特征向量中新增一个第零分量，把它设为1；

2.特征缩放
  -数据集中的向量分量量级可能差距较大，我们通过特征缩放将它们处理成为同一个量级；
  -特征缩放可以使得梯度下降更快找到最优解；
  -将每一个特征值缩放到大约-1到1的范围内；
  
  
  
  


  
  
