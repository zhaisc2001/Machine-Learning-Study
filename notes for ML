第一章
1.机器学习应用实例：
  -数据挖掘
  -完成我们无法编程完成的工作（计算机视觉、自然语言处理）
  -私人定制程序
  -对人类学习方式有更深入的理解

2.什么是机器学习？
  -Arthur Samuel定义：让计算机能够在未被明确编程的情况下自我进行学习的研究；
  -Tom Michell定义：一个在经验E中学习，解决任务T，被使用P指标来量化表现的计算机程序；
  对于Samuel的跳棋程序来说，经验E指跳棋自对弈的成千上万把比赛，任务T指进行跳棋对弈，指标P指与新对手对弈的胜率（新对手！）；
  
3.机器学习算法
  -主要是监督学习与无监督学习；
  -其他的还有强化学习和推荐系统；
  
4.监督学习
  -监督学习是包含了正确答案的数据集，使用这个算法的目的是给出更多的正确答案；
  -输出是连续的：回归问题；输出是离散的：分类问题；
  
5.无监督学习
  -无监督学习中数据集没有标签，使用该算法将数据分类成不同的簇；
  -聚类算法：将属性相近的个体归类到一起；
  -应用：鸡尾酒派对问题算法；

第二章 单变量线性回归
1.模型描述
  -m通常用来代表训练集的数量，x代表输入变量，y代表输出变量，h表示假设函数；

2.代价函数
  -解决最小化问题，选择参数使得残差平方极小；
  -代价函数写法；
  
3.代价函数（一）
  -线性回归函数的优化目标；
  
4.代价函数（二）
  -带一个参数的代价函数图像为二维的；
  -带两个参数的代价函数图像为三维的；
  -代价函数的最小值时的参数取值是回归时的最优取值；
  
5.梯度下降
  -在每一个步长点找到斜率最小的方向进行下降；
  -进行梯度下降需要起始点；
  -代价函数前的系数称为学习率；
  -梯度下降的最重要一点是同步更新；
  -学习率小，更新速率小，但一定程度上能保证参数收敛；学习率大，更新速率大，但很可能参数不收敛；
  -在收敛至局部最优点附近时，更新速率会越来越小，这是由于导数值越来越接近0导致的；

6.线性回归中的梯度下降
  -梯度下降代价函数是一个凸函数，有且只有一个全局最优解，而且使用梯度下降的方法总能收敛到此处；
  -像这样遍历了整个数据集的梯度下降法，我们称之为“Batch梯度下降法”；
  
第三章 多变量线性回归
1.多功能
  -此时输入的变量是一个向量；
  -在特征向量中新增一个第零分量，把它设为1；

2.特征缩放
  -数据集中的向量分量量级可能差距较大，我们通过特征缩放将它们处理成为同一个量级；
  -特征缩放可以使得梯度下降更快找到最优解；
  -将每一个特征值缩放到大约-1到1的范围内；
  -在[-3,-1/3]U[1/3,3]范围内属于正常；
  
3.均值归一化
  -将特征值更换为特征值减去平均值除以样本取值范围；
  -通过观察梯度下降曲线，我们可以检测算法是否正常工作，正常情况下，随着迭代次数的增加，代价函数的值会降低；
  -当代价函数图像呈现增函数趋势时，通常是由于学习率过大导致的；
  
ex1:
梯度下降法适用各种类型的回归模型，需要选择学习率以及初始参数；而正规方程法只需要计算，不需要选择学习率，有θ=(XTX)−1XTy。 

第四章 Logistic 回归
1.分类
  -分类属于监督学习的一种，它的输出是离散的；
  -线性回归在分类方面并不是一个良好的方法；
  -Logistic回归的输出都在0和1之间；
  
2.假设陈述
  -假设函数；
  -sigmoid函数或者logistic函数值的都是同一个函数：f(x)=1/(1+e^(-x));
  -在logistic回归中，假设函数的输出表示为在给定x和theta的情况下，y=1的概率为多少；
  
3.决定界限
  -通过对logistic函数的可视化，我们可以看到当x>0时，f(x)>0.5,反之f(x)<0.5;
  -logistic函数中的未知数可以是多项式，这样我们的决定界限可以是一个很复杂的图像；
  
4.代价函数
  -使用线性回归的代价函数会导致logistic回归的代价函数变成一个拥有多个局部最小值的非凸函数；
  -logictic回归的代价函数：当y=1时，cost(h,y)=-lg(h(x));
                        当y=0时，cost(h,y)=-lg(1-h(x));

5.简化代价函数与梯度下降
  -cost(h,y)=y*-lg(h(x))+(1-y)*-lg(1-h(x));
  
6.多类别分类问题
  -n分类问题在机器学习中我们通常将它转化成为n个独立的二分类问题，通过创造伪训练集进行训练；
  
第五章 正则化
1.过拟合问题
  -欠拟合会导致高偏差，过拟合会导致高方差，二者都会使模型的泛化能力降低；
  -过拟合指在过多的变量下，可能导致模型能够很好地拟合我们的训练集，但不能拟合新的数据；
  -解决过拟合的方法：1.减少变量的数量；（人工选择或是通过算法进行选择）
                  2.进行正则化：保留所有变量，但减少变量的量级；适用于每个变量都对预测结果有些许贡献的模型；

2.代价函数
  -通过在代价函数中增添参数的惩罚项，可以使某些参数的数值接近于0；
  -通过在代价函数中增添一项正未知系数乘以系数平方和，可以使参数的数值都减小；
  
3.正则化
  -线性回归的正则化：θ=(XTX+λA)−1XTy;A为n+1维的单位矩阵，第一个元素为0;
  -logistic回归的正则化：除了代价函数改变之外，梯度下降并未改变；
 
第六章 神经网络学习
1.非线性假设
  -当特征非常多的时候，使用线性分类器来进行分类不是一个好主意；
  -在计算机视觉中，由于像素过多，我们通常使用神经网络学习算法；
  
2.模型展示
  -我们将神经网络模型中的基本单元称为逻辑单元，具有输入节点和输出节点
  通过函数计算输入的值，得到输出结果；
  -神经网络模型中的激活函数通常是指sigmoid函数；
  -神经网络模型中第一层称为输入层，最后一层称为输出层，中间称为隐藏层，隐藏层可以有多层；
  -从输入层到隐藏层再到输出层，这样的传播过程叫做前向传播；
  -通过对权重矩阵中元素值的改变，我们可以使神经网络实现逻辑操作；
  
3.反向传播
  -反向传播算法需要计算误差值，误差值=激活值-实值；
  -当计算出输出层的误差值后，我们可以通过公式计算隐藏层的误差值，但一般我们不计算输入层的误差值；
  
  

  
  
  
  


  
  
