第一章
1.机器学习应用实例：
  -数据挖掘
  -完成我们无法编程完成的工作（计算机视觉、自然语言处理）
  -私人定制程序
  -对人类学习方式有更深入的理解

2.什么是机器学习？
  -Arthur Samuel定义：让计算机能够在未被明确编程的情况下自我进行学习的研究；
  -Tom Michell定义：一个在经验E中学习，解决任务T，被使用P指标来量化表现的计算机程序；
  对于Samuel的跳棋程序来说，经验E指跳棋自对弈的成千上万把比赛，任务T指进行跳棋对弈，指标P指与新对手对弈的胜率（新对手！）；
  
3.机器学习算法
  -主要是监督学习与无监督学习；
  -其他的还有强化学习和推荐系统；
  
4.监督学习
  -监督学习是包含了正确答案的数据集，使用这个算法的目的是给出更多的正确答案；
  -输出是连续的：回归问题；输出是离散的：分类问题；
  
5.无监督学习
  -无监督学习中数据集没有标签，使用该算法将数据分类成不同的簇；
  -聚类算法：将属性相近的个体归类到一起；
  -应用：鸡尾酒派对问题算法；

第二章 单变量线性回归
1.模型描述
  -m通常用来代表训练集的数量，x代表输入变量，y代表输出变量，h表示假设函数；

2.代价函数
  -解决最小化问题，选择参数使得残差平方极小；
  -代价函数写法；
  
3.代价函数（一）
  -线性回归函数的优化目标；
  
4.代价函数（二）
  -带一个参数的代价函数图像为二维的；
  -带两个参数的代价函数图像为三维的；
  -代价函数的最小值时的参数取值是回归时的最优取值；
  
5.梯度下降
  -在每一个步长点找到斜率最小的方向进行下降；
  -进行梯度下降需要起始点；
  -代价函数前的系数称为学习率；
  -梯度下降的最重要一点是同步更新；
  -学习率小，更新速率小，但一定程度上能保证参数收敛；学习率大，更新速率大，但很可能参数不收敛；
  -在收敛至局部最优点附近时，更新速率会越来越小，这是由于导数值越来越接近0导致的；

6.线性回归中的梯度下降
  -梯度下降代价函数是一个凸函数，有且只有一个全局最优解，而且使用梯度下降的方法总能收敛到此处；
  -像这样遍历了整个数据集的梯度下降法，我们称之为“Batch梯度下降法”；
  
第三章 多变量线性回归
1.多功能
  -此时输入的变量是一个向量；
  -在特征向量中新增一个第零分量，把它设为1；

2.特征缩放
  -数据集中的向量分量量级可能差距较大，我们通过特征缩放将它们处理成为同一个量级；
  -特征缩放可以使得梯度下降更快找到最优解；
  -将每一个特征值缩放到大约-1到1的范围内；
  -在[-3,-1/3]U[1/3,3]范围内属于正常；
  
3.均值归一化
  -将特征值更换为特征值减去平均值除以样本取值范围；
  -通过观察梯度下降曲线，我们可以检测算法是否正常工作，正常情况下，随着迭代次数的增加，代价函数的值会降低；
  -当代价函数图像呈现增函数趋势时，通常是由于学习率过大导致的；
  
ex1:
梯度下降法适用各种类型的回归模型，需要选择学习率以及初始参数；而正规方程法只需要计算，不需要选择学习率，有θ=(XTX)−1XTy。 

第四章 Logistic 回归
1.分类
  -分类属于监督学习的一种，它的输出是离散的；
  -线性回归在分类方面并不是一个良好的方法；
  -Logistic回归的输出都在0和1之间；
  
2.假设陈述
  -假设函数；
  -sigmoid函数或者logistic函数值的都是同一个函数：f(x)=1/(1+e^(-x));
  -在logistic回归中，假设函数的输出表示为在给定x和theta的情况下，y=1的概率为多少；
  
3.决定界限
  -通过对logistic函数的可视化，我们可以看到当x>0时，f(x)>0.5,反之f(x)<0.5;
  -logistic函数中的未知数可以是多项式，这样我们的决定界限可以是一个很复杂的图像；
  
4.代价函数
  -使用线性回归的代价函数会导致logistic回归的代价函数变成一个拥有多个局部最小值的非凸函数；
  -logictic回归的代价函数：当y=1时，cost(h,y)=-lg(h(x));
                        当y=0时，cost(h,y)=-lg(1-h(x));

5.简化代价函数与梯度下降
  -cost(h,y)=y*-lg(h(x))+(1-y)*-lg(1-h(x));
  
6.多类别分类问题
  -n分类问题在机器学习中我们通常将它转化成为n个独立的二分类问题，通过创造伪训练集进行训练；
  
第五章 正则化
1.过拟合问题
  -欠拟合会导致高偏差，过拟合会导致高方差，二者都会使模型的泛化能力降低；
  -过拟合指在过多的变量下，可能导致模型能够很好地拟合我们的训练集，但不能拟合新的数据；
  -解决过拟合的方法：1.减少变量的数量；（人工选择或是通过算法进行选择）
                  2.进行正则化：保留所有变量，但减少变量的量级；适用于每个变量都对预测结果有些许贡献的模型；

2.代价函数
  -通过在代价函数中增添参数的惩罚项，可以使某些参数的数值接近于0；
  -通过在代价函数中增添一项正未知系数乘以系数平方和，可以使参数的数值都减小；
  
3.正则化
  -线性回归的正则化：θ=(XTX+λA)−1XTy;A为n+1维的单位矩阵，第一个元素为0;
  -logistic回归的正则化：除了代价函数改变之外，梯度下降并未改变；
 
第六章 神经网络学习
1.非线性假设
  -当特征非常多的时候，使用线性分类器来进行分类不是一个好主意；
  -在计算机视觉中，由于像素过多，我们通常使用神经网络学习算法；
  
2.模型展示
  -我们将神经网络模型中的基本单元称为逻辑单元，具有输入节点和输出节点
  通过函数计算输入的值，得到输出结果；
  -神经网络模型中的激活函数通常是指sigmoid函数；
  -神经网络模型中第一层称为输入层，最后一层称为输出层，中间称为隐藏层，隐藏层可以有多层；
  -从输入层到隐藏层再到输出层，这样的传播过程叫做前向传播；
  -通过对权重矩阵中元素值的改变，我们可以使神经网络实现逻辑操作；
  
3.反向传播
  -反向传播算法需要计算误差值，误差值=激活值-实值；
  -当计算出输出层的误差值后，我们可以通过公式计算隐藏层的误差值，但一般我们不计算输入层的误差值；
  
第七章 应用机器学习的建议
1.决定下一步做什么
  -当机器学习的结果不令人满意时，我们可以
    -试图得到更多的训练集；
    -尝试使用更少的特征防止过拟合；
    -使用更多的特征防止欠拟合；
    -增加多项式特征；
    -增大或减小正则化参数lamda的值；
    
2.评估假设
  -过小的训练误差不一定意味着好的机器学习算法，有过拟合的可能；
  -通过将数据集随机划分为训练集和测试集的方法，我们可以对算法的泛化能力进行测试；（一般是七三划分）
  -对于回归类问题，我们可以使用测试集的代价函数来评估算法；
  -对于分类问题，我们可以使用0/1分类误差来度量算法准确性；
  
3.模型选择和训练、验证、测试集
  -在模型选择中我们将数据集分为三个部分，即训练、验证、测试，比例为6:2:2；
  -通过对交叉验证集误差的计算，我们将可以选择参数多项式最高次数的多少；
  
4.诊断方差和偏差；
  -方差过大意味着过拟合，偏差过大意味着欠拟合；
  -过拟合时只有交叉验证的误差较大，训练误差是比较小的；
  -欠拟合时交叉验证和训练误差都较大；
  
5.正则化和偏差、方差
  -正则化参数过大会导致欠拟合，过小到只过拟合；
  -在计算误差时，我们不考虑正则化；

6.学习曲线
  -横轴为训练集样本数量，纵轴为训练集误差和交叉验证集误差；
  -训练集样本量较小时，交叉验证误差大，训练误差小，此时存在欠拟合问题；
  -训练集样本量较大时，交叉验证误差减小，训练误差增大，存在过拟合问题；
  
7.决定接下来做什么
  -获取更多训练集样本：解决欠拟合；
  -尝试使用更少的特征：解决过拟合；
  -使用更多的特征：解决欠拟合；
  -增加多项式特征：解决欠拟合；
  -增大正则化参数：解决欠拟合；
  -减小正则化参数：解决过拟合；
  

第八章 机器学习系统设计

1.确定执行的优先级
  -假如建立一个垃圾邮件分类器，如何提高精度？
    -收集大量数据；
    -通过发现特征（奇怪的标题、复杂的服务器等等）
    -使用数字代替字母表示屏蔽词；

2.误差分析
  -在进行机器学习项目时，首先要做的不是寻找复杂的特征，而是通过一个简单的算法快速实现项目；
  再通过绘制学习曲线，观察是否存在过拟合或欠拟合问题；
  -垃圾邮件分类器的误差分析：假如交叉验证集有500个样本，通过算法去对100个样本进行分类，
  再通过手动分类，判定误差的来源；
  -垃圾邮件特征：故意的拼写错误；奇怪的邮件标题；不常见的标点符号使用；
  -对于不同算法而言，最难处理的部分总是相同的，所以我们使用快速实现项目的方式再进行误差分析，
  从而找到最难处理的部分；
  -误差分析建议在交叉验证集上进行而不在测试集上进行；
  
3.非对称性分类误差分析
  -当各类数量极为不平衡的时候，我们使用误差来衡量算法性质不大理想；
  -引入查准率和召回率；查准率 = 真阳性/预测阳性；召回率 = 真阳性/实际所有阳性；


第九章 支持向量机
1.优化目标
  -通过对logistic回归代价函数的改动可以得到SVM算法的代价函数；
  -在SVM算法代价函数中，人们遵循的惯例是不去除以m；
  -在SVM算法中，原来的正则化系数去掉，新的正则化系数C添加在非正则化项前面；
  -支持向量机与logitic回归不同，它是直接给出分类结果，而不是一个概率；
  
2.直观上大间隔的理解
  -由于SVM算法设置的类别划分门槛较高，因此它提供的决策边界会更加稳健，更具有鲁棒性；
  -由于这种大间隔特点，当正则化参数较大时，异常点对SVM算法结果的影响会非常明显；
  -正则化参数C的作用相当于1/lamda;
 
3.大间隔分类器的数学原理
  -简单来说，当决策边界与样本点距离较小时，会导致算法与优化目标冲突，因此SVM算法得到的决策边界
  倾向于远离样本点；
  
4.核函数
  -我们通过使用标记点和相似度函数（核函数）来构造新特征，以便对线性不可分的数据集进行分类；
  -高斯核函数参数较大，会导致高偏差，低方差；
  
5.使用SVM
  -参数C的选择以及核函数的选择：当不使用核函数时，SVM会生成线性分类器；
  -高斯核函数和线性函数是最常使用的两种核函数；
  -使用高斯核函数，需要将训练集数据标准化；
  -除了上述两种核函数以外，还有多项式核函数等不常使用的核函数；
  -svm的k分类问题也有ovo与ovr等参数；
  -当训练集较小时，一般选择不带核函数（线性核函数）；
  训练集较大时，选择核函数；

第十章 无监督学习
1.无监督学习
  -没有任何标签信息；
  -市场分析，社交网络分析，计算机集群，了解银河系构成；
  
2.K-Means算法
  -步骤：1）随机生成两点，这两点作为样本中心，遍历每个样本点，根据距离样本中心的远近将样本点分为两类；
        2）移动样本中心至第一步中分类的点坐标均值处；
        3）重复第一步（除去生成随机点的环节）与第二步，直到每个样本的类别都不再改变为止；
        
3.优化目标
  -代价函数就是每个样本离自己的样本中心距离的平方和；
  
4.随即初始化
  -可以随机挑选样本点作为样本中心；
  -初始化状态不同，可能导致最后分类结果的不同，这是因为算法得到的是局部最优值；

5.选取聚类数量
  -通常是手动选取；
  -“肘部法则”：使用不同的K值进行聚类，得到K-Cost曲线，突变值最大的点我们将他作为选取的
  聚类数量；
  -根据后续目的选择聚类数量，如市场分割、计算机集群等；

第十一章 降维
1.数据压缩
  -减少特征：将数据由二维（平面）降至一维（线），可以提供更好分析的特征；
2.可视化
  -为了方便可视化，我们会把几十维的特征向量降低到三维甚至二维；
3.主成分分析方法（PCA）
  -PCA将数据降维，投影到低维平面时，得到的结果所有样本点到低维平面投影误差的平方和最小；
  -PCA与线性回归中定义的误差是不同的，PCA中是点到线的垂直距离，线性回归只是点到点的距离；
  -PCA需要对数据进行预处理，需要将数据规则标准化
  
